{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import gc\n",
    "import sys,os\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from scipy import stats\n",
    "\n",
    "# np.set_printoptions(suppress=True)\n",
    "# pd.set_option('colwidth',50)\n",
    "# pd.set_option('max_rows',50)\n",
    "\n",
    "from sklearn.model_selection import GroupKFold,KFold,StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel, BertTokenizer,BertPreTrainedModel,BertConfig,BertTokenizerFast\n",
    "from transformers import RobertaTokenizer,RobertaTokenizerFast,RobertaModel,RobertaConfig\n",
    "from transformers import get_linear_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    rn.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(8421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n",
    "sub_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "print(train_df.loc[8729])\n",
    "print(train_df.loc[21376])\n",
    "train_df.drop(8729,inplace=True) # selecttext 不对\n",
    "\n",
    "# train_df.drop(21376,inplace=True)\n",
    "\n",
    "# train_df = pd.read_csv('/kaggle/input/selactect-extraction-get-new/train.csv')\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df['selected_text'] = test_df['text']\n",
    "train_df.shape,test_df.shape,sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[train_df['selected_text'].apply(lambda x:len(x.split())==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_df.columns,test_df.columns,sub_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, textID, tokenizer, max_len):\n",
    "\n",
    "    # 处理selected_text 的首字符\n",
    "#     ori_selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "#     x1 = str(selected_text).split()\n",
    "#     selected_text = \" \" + \" \".join(x1)\n",
    "    \n",
    "    ori_selected_text = str(selected_text)\n",
    "    selected_text = ori_selected_text\n",
    "    \n",
    "    ori_tweet = str(tweet)\n",
    "    tweet = ori_tweet\n",
    "    # --------\n",
    "#     ori_tweet = \" \" + \" \".join(str(tweet).split())\n",
    "#     tweet = ori_tweet\n",
    "\n",
    "    \n",
    "    # 清洗http\n",
    "    def clean_text(text,clean_offset):\n",
    "        text_list = list(text)\n",
    "        while re.search('(\\shttps?:?//\\S+)|(\\swww\\.\\S+)',''.join(text_list)):\n",
    "            old_s,old_e = re.search('(\\shttps?:?//\\S+)|(\\swww\\.\\S+)',''.join(text_list)).span()\n",
    "            new_len = len(' http')\n",
    "            clean_offset[old_s+new_len-1] = clean_offset[old_e-1]\n",
    "            for i in range(old_e-1,old_s+new_len-1,-1):\n",
    "                clean_offset.pop(i)\n",
    "                text_list.pop(i)\n",
    "            text_list[old_s:old_s+new_len] = list(' http')\n",
    "        \n",
    "        assert len(clean_offset)==len(text_list)\n",
    "        return ''.join(text_list),clean_offset\n",
    "\n",
    "    clean_offset = list(range(len(tweet)))\n",
    "#     tweet, clean_offset = clean_text(tweet,clean_offset)\n",
    "#     tweet = re.sub(\"'\",'`',tweet)\n",
    "    \n",
    "    # 这样的分割在空格上是模糊地带，输出可能带有移位，需要靠另一个函数来矫正输出\n",
    "    def separate_alphanum(text,offset):\n",
    "        assert len(text)==len(offset)\n",
    "        outstr = text[0]\n",
    "        for i,char in enumerate(text[1:],start=1):\n",
    "            if text[i-1].isspace() or char.isspace():\n",
    "                outstr += char\n",
    "                continue\n",
    "            if text[i-1].isalpha() and char.isalpha():\n",
    "                outstr += char\n",
    "                continue\n",
    "            if text[i-1].isdigit() and char.isdigit():\n",
    "                outstr += char\n",
    "                continue\n",
    "            if (not text[i-1].isalnum()) and (not char.isalnum()):\n",
    "                outstr += char\n",
    "                continue\n",
    "            outstr += ' '\n",
    "            outstr += char\n",
    "        i = len(outstr)-1\n",
    "        j = len(text)-1\n",
    "        while i>=0:\n",
    "            if outstr[i]!=text[j]:\n",
    "                assert outstr[i]==' '\n",
    "                offset.insert(j+1,offset[j+1])\n",
    "                i-=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "        return outstr,offset\n",
    "\n",
    "#     tweet, clean_offset = separate_alphanum(tweet,\n",
    "#                                             clean_offset)\n",
    "\n",
    "#     restra = ''.join([ori_tweet[i] for i in clean_offset])\n",
    "\n",
    "#     print(ori_tweet)\n",
    "#     print(tweet)\n",
    "#     print(ori_selected_text)\n",
    "#     print(selected_text)\n",
    "#     print(clean_offset)\n",
    "    \n",
    "    # offset 是要相对于原始字符串\n",
    "    # idx0 是用于原始位置对应的 新字符串的位置\n",
    "    # 原始的selected_text 和原始text 找标记，然后offset重定位,获取新selected_text\n",
    "    def get_new_selectext(selectext, text, new_text, offset):\n",
    "        len_st = len(selectext) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for ind in (i for i, e in enumerate(text) if e == selectext[1]):\n",
    "            if \" \" + text[ind: ind+len_st] == selectext:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "        for i,v in enumerate(offset):\n",
    "            if idx0<=v:\n",
    "                idx0 = i\n",
    "                break\n",
    "        for i,v in enumerate(offset):\n",
    "            if idx1<=v:\n",
    "                idx1 = i\n",
    "                break  \n",
    "        return \" \" + new_text[idx0:idx1+1]\n",
    "\n",
    "#     selected_text = get_new_selectext(selected_text,\n",
    "#                                                 ori_tweet,\n",
    "#                                                 tweet,\n",
    "#                                                 clean_offset)\n",
    "\n",
    "    #然后可以开始矫正selected_text\n",
    "    def align_selectext(selectext,text):\n",
    "        t = text.split()\n",
    "        st = selectext.split()\n",
    "        out_str = []\n",
    "        get = False\n",
    "        for i,vt in enumerate(t):\n",
    "            if (st[0] in vt) and (len(st)+i-1)<len(t):\n",
    "                for j,vst in enumerate(st):\n",
    "                    if vst not in t[i+j]:\n",
    "                        get = False\n",
    "                        break\n",
    "                    get =True\n",
    "                if get:\n",
    "                    \n",
    "                    for j,vst in enumerate(st):\n",
    "                        if vst!=t[i+j] :\n",
    "                            if len(vst)>=len(t[i+j])/2 or len(st)<2:\n",
    "                                out_str.append(t[i+j])\n",
    "                            else:\n",
    "                                continue\n",
    "                        elif vst==t[i+j]:\n",
    "                            out_str.append(t[i+j])\n",
    "                    break\n",
    "        if not get:\n",
    "            raise\n",
    "        else:\n",
    "            return \" \" + ' '.join(out_str)\n",
    "\n",
    "#     selected_text = align_selectext(selected_text, \n",
    "#                                     tweet)\n",
    "#     print(repr(selected_text))\n",
    "#     print(repr(tweet))\n",
    "    def get_sted(selectext, text):\n",
    "        len_st = len(selectext) \n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for ind in (i for i, e in enumerate(text) if e == selectext[0]):\n",
    "            if text[ind: ind+len_st] == selectext:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st-1\n",
    "                break\n",
    "        if not idx1:\n",
    "            raise\n",
    "        return idx0,idx1\n",
    "    \n",
    "    idx0,idx1 = get_sted(selected_text,\n",
    "                         tweet)\n",
    "#     print(tweet)\n",
    "#     print(selected_text)\n",
    "    # 根据tokenizer的offset，计算sted\n",
    "    char_targets = [0] * len(tweet)   # 维护一个select标记等长序列\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    else:\n",
    "        raise\n",
    "#     print(char_targets)\n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "#     ss = ' '.join([tokenizer.id_to_token(i) for i in tok_tweet.ids])\n",
    "#     print(repr(ss))\n",
    "#     print(repr(tokenizer.decode(tok_tweet.ids)))\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "            \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "#     print(tokenizer.id_to_token(tok_tweet.ids[targets_start]))\n",
    "#     print(tokenizer.id_to_token(tok_tweet.ids[targets_end]))\n",
    "    \n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    " \n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "\n",
    "#     print(tweet[tweet_offsets[targets_start][0]:tweet_offsets[targets_end][1]])\n",
    "    tweet_offsets = [(clean_offset[s],clean_offset[e-1]+1) if e!=0 else (0,0) for s,e in tweet_offsets]\n",
    "#     print(tweet_offsets)\n",
    "#     print(tweet_offsets[targets_start][0],tweet_offsets[targets_end][1])\n",
    "    if tweet_offsets[targets_start] ==(0,0):\n",
    "        print('offset error atart')\n",
    "    if tweet_offsets[targets_end] ==(0,0):\n",
    "        print('offset error atart')\n",
    "#     print(tweet_offsets[targets_start])\n",
    "#     print(repr(ori_tweet[tweet_offsets[targets_start][0]:tweet_offsets[targets_end][1]]))\n",
    "#     print(repr(ori_selected_text))\n",
    "    encoded_dict = {'input_ids':input_ids,\n",
    "                   'token_type_ids':token_type_ids,\n",
    "                    'attention_mask':mask,\n",
    "                    'sentiment': sentiment,  # Sentiment_to_Num\n",
    "                    'offset_mapping':tweet_offsets,\n",
    "#                     'clean_offset':clean_offset,\n",
    "                    'textID':textID,\n",
    "                    'text':ori_tweet,\n",
    "                    'start_position':targets_start,\n",
    "                    'end_position':targets_end,\n",
    "                    'selected_text':ori_selected_text,\n",
    "                   }\n",
    "    return encoded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class SpanDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_df, max_seq_length=256, is_raw=True):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.is_raw = is_raw\n",
    "        \n",
    "        print('dataset len:',self.__len__())\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            if self.is_raw:\n",
    "                data = process_data(\n",
    "                    self.data_df.loc[index,'text'], \n",
    "                    self.data_df.loc[index,'selected_text'],\n",
    "                    self.data_df.loc[index,'sentiment'],\n",
    "                    self.data_df.loc[index,'textID'],\n",
    "                    self.tokenizer, \n",
    "                    self.max_seq_length,\n",
    "                )\n",
    "            else:\n",
    "                data = process_data(\n",
    "                    self.data_df.loc[index,'text'], \n",
    "                    self.data_df.loc[index,'new_selectext'],\n",
    "                    self.data_df.loc[index,'sentiment'],\n",
    "                    self.data_df.loc[index,'textID'],\n",
    "                    self.tokenizer, \n",
    "                    self.max_seq_length,\n",
    "                )\n",
    "        except:\n",
    "            print('data error',index)\n",
    "            raise\n",
    "            return\n",
    "\n",
    "        encoded_dict = {'input_ids': torch.tensor(data[\"input_ids\"], dtype=torch.long),\n",
    "                       'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "                        'attention_mask': torch.tensor(data[\"attention_mask\"], dtype=torch.long),\n",
    "\n",
    "                        'offset_mapping': torch.tensor(data[\"offset_mapping\"], dtype=torch.int),\n",
    "                        'textID': data[\"textID\"],\n",
    "                        'text':data['text'],\n",
    "                        \n",
    "                        'sentiment':data['sentiment'],\n",
    "                        'start_position':torch.tensor(data[\"start_position\"], dtype=torch.long),\n",
    "                        'end_position':torch.tensor(data[\"end_position\"], dtype=torch.long),\n",
    "                        'selected_text':data['selected_text'],\n",
    "                       }\n",
    "\n",
    "        return encoded_dict\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "#         vocab_file=\"/kaggle/input/roberta-base/vocab.json\", \n",
    "#         merges_file=\"/kaggle/input/roberta-base/merges.txt\", \n",
    "#         lowercase=True,\n",
    "#         add_prefix_space=True\n",
    "#     )\n",
    "# test_dataset = SpanDataset(tokenizer,\n",
    "#                             train_df,\n",
    "#                             96)\n",
    "# def get_length(text):\n",
    "#     tok_tweet = tokenizer.encode(text)\n",
    "#     input_ids_orig = tok_tweet.ids\n",
    "#     tweet_offsets = tok_tweet.offsets\n",
    "#     return len(input_ids_orig)\n",
    "# train_df['enc_len'] = train_df['text'].apply(get_length)\n",
    "# train_df['enc_len'].describe()\n",
    "# for i in range(len(test_dataset)):\n",
    "#     test_dataset[i]\n",
    "# 8728\\26004\n",
    "# test_dataset[258]\n",
    "# test_dataset[5]\n",
    "# 5696\\6112\n",
    "# test_dataset[5696]\n",
    "# test_dataset[6112]\n",
    "# test_dataset[18]\n",
    "# test_dataset.error_num\n",
    "# test_dataset[26]\n",
    "# test_dataset[1900]  # token 分割句子，那末尾字符也是分配在一个token内\n",
    "# 3621/5188/15205/  # 21374被删除\n",
    "# test_dataset[8267]\n",
    "# test_dataset[3754]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class SpanBert(BertPreTrainedModel):  # 重写\n",
    "    def __init__(self, config, model, PTM_path):\n",
    "        config.output_hidden_states = True\n",
    "        super(SpanBert, self).__init__(config)\n",
    "        \n",
    "        self.bert = model.from_pretrained(PTM_path, config=config)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "#         self.liner_to_num_labels = nn.Linear(config.hidden_size*2, 2)  # start/end\n",
    "        \n",
    "        self.liner_to_start = nn.Linear(config.hidden_size*2, 1)\n",
    "        self.liner_to_end = nn.Linear(config.hidden_size*4, 1)\n",
    "#         n_weights = config.num_hidden_layers + 1\n",
    "#         weights_init = torch.zeros(n_weights).float()\n",
    "#         weights_init[:-1] = -3   # 咋想的\n",
    "#         self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "        \n",
    "#         self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "    ):\n",
    "        # one for the output of the embeddings + one for the output of each layer\n",
    "        _,_,out = self.bert(   # batch size, seq_size, hid_dim\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "#         sequence_output = torch.stack(   # torch.Size([1, 148, 1024, 25])\n",
    "#             [self.dropout(layer) for layer in out_],\n",
    "#             dim=3\n",
    "#         )\n",
    "#         sequence_output = (      # torch.Size([1, 148, 1024])\n",
    "#             torch.softmax(self.layer_weights, dim=0) * sequence_output\n",
    "#         ).sum(-1)\n",
    "\n",
    "#         logits = self.liner_to_num_labels(out)\n",
    "#         start_logits, end_logits = logits.split(1, dim=-1)\n",
    "#         start_logits = start_logits.squeeze(-1)\n",
    "#         end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        start_logits = self.liner_to_start(out)\n",
    "        start_token = torch.gather(out,1,start_logits.argmax(dim=1, keepdim=True).repeat(1,1,out.size(2))) # 在某一轴上自由index\n",
    "        out2 = torch.cat([out,start_token.repeat(1,out.size(1),1)], dim=2)\n",
    "        end_logits = self.liner_to_end(out2)\n",
    "        \n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
    "    \n",
    "# model = SpanBert(config = Model_Class[args.model_name][0].from_pretrained('/kaggle/input/roberta-base'),\n",
    "#                  model = Model_Class['roberta'][2],\n",
    "#                  PTM_path = '/kaggle/input/roberta-base')\n",
    "\n",
    "# model(input_ids=torch.tensor([258,369,456,156,896,845,812,123]).view(2,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_model_optimizer(model,args):\n",
    "    \n",
    "    params = list(model.named_parameters())\n",
    "    no_decay = [\"bias\",\"LayerNorm.bias\",\"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in params if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.lr,weight_decay=0)\n",
    "\n",
    "    return optimizer\n",
    "# model = CustomBert.from_pretrained(args.bert_model)\n",
    "# opti = get_model_optimizer(model)\n",
    "# opti.state_dict()['param_groups'][0]['lr'] #state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(start_preds, end_preds, start_labels, end_labels):\n",
    "    start_loss = nn.CrossEntropyLoss()(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss()(end_preds, end_labels)\n",
    "    return start_loss + end_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_output_string(text, offset, pred_st, pred_ed):\n",
    "\n",
    "    if pred_st>pred_ed:\n",
    "        return text\n",
    "    pred_str = text[offset[pred_st][0] : offset[pred_ed][1]]\n",
    "    # 不晓得哪里报的错\n",
    "    if len(pred_str.split())==0:\n",
    "        return text\n",
    "    \n",
    "#     pred_str = xiuzhen_str(pred_str,text)\n",
    "    return pp(pred_str,text)\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    score = float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pp(filtered_output, real_tweet):\n",
    "    filtered_output = ' '.join(filtered_output.split())\n",
    "    if len(real_tweet.split()) < 2:\n",
    "        filtered_output = real_tweet\n",
    "    else:\n",
    "#         print(filtered_output)\n",
    "        if len(filtered_output.split()) == 1:\n",
    "            if filtered_output.endswith(\"..\"):\n",
    "                if real_tweet.startswith(\" \"):\n",
    "                    st = real_tweet.find(filtered_output)\n",
    "                    fl = real_tweet.find(\"  \")\n",
    "                    if fl != -1 and fl < st:\n",
    "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n",
    "                    else:\n",
    "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
    "                else:\n",
    "                    st = real_tweet.find(filtered_output)\n",
    "                    fl = real_tweet.find(\"  \")\n",
    "                    if fl != -1 and fl < st:\n",
    "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n",
    "                    else:\n",
    "                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n",
    "                return filtered_output\n",
    "            if filtered_output.endswith('!!'):\n",
    "                if real_tweet.startswith(\" \"):\n",
    "                    st = real_tweet.find(filtered_output)\n",
    "                    fl = real_tweet.find(\"  \")\n",
    "                    if fl != -1 and fl < st:\n",
    "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n",
    "                    else:\n",
    "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
    "                else:\n",
    "                    st = real_tweet.find(filtered_output)\n",
    "                    fl = real_tweet.find(\"  \")\n",
    "                    if fl != -1 and fl < st:\n",
    "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n",
    "                    else:\n",
    "                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n",
    "                return filtered_output\n",
    "\n",
    "        if real_tweet.startswith(\" \"):\n",
    "            filtered_output = filtered_output.strip()\n",
    "            text_annotetor = ' '.join(real_tweet.split())\n",
    "            start = text_annotetor.find(filtered_output)\n",
    "            end = start + len(filtered_output)\n",
    "            start -= 0\n",
    "            end += 2\n",
    "            flag = real_tweet.find(\"  \")\n",
    "            if flag < start:\n",
    "                filtered_output = real_tweet[start:end]\n",
    "\n",
    "        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n",
    "            filtered_output = filtered_output.strip()\n",
    "            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n",
    "            start = text_annotetor.find(filtered_output)\n",
    "            end = start + len(filtered_output)\n",
    "            start -= 0\n",
    "            end += 2\n",
    "            flag = real_tweet.find(\"  \")\n",
    "            if flag < start:\n",
    "                filtered_output = real_tweet[start:end]\n",
    "    return filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xiuzhen_str(pred,text):\n",
    "    t = text.split()\n",
    "    st = pred.split()\n",
    "    out_str = []\n",
    "    get = False\n",
    "    for i,vt in enumerate(t):\n",
    "        if (st[0] in vt) and (len(st)+i-1)<len(t):\n",
    "            for j,vst in enumerate(st):\n",
    "                if vst not in t[i+j]:\n",
    "                    get = False\n",
    "                    break\n",
    "                get =True\n",
    "            if get:\n",
    "                for j,vst in enumerate(st):\n",
    "                    out_str.append(t[i+j])\n",
    "                break\n",
    "    if not get:\n",
    "        raise\n",
    "    else:\n",
    "        return ' '.join(out_str)\n",
    "def get_wrong_str(pred,text,selected_text):\n",
    "    t = text.split()\n",
    "    st = pred.split()\n",
    "    wrong_str = []\n",
    "    get = False\n",
    "    for i,vt in enumerate(t):\n",
    "        if (st[0] in vt) and (len(st)+i-1)<len(t):\n",
    "            for j,vst in enumerate(st):\n",
    "                if vst not in t[i+j]:\n",
    "                    get = False\n",
    "                    break\n",
    "                get =True\n",
    "            if get:\n",
    "                for j,vst in enumerate(st):\n",
    "                    if vst in t[i+j] and vst!=t[i+j]:\n",
    "                        wrong_str.append((repr(vst),repr(t[i+j])))\n",
    "                break\n",
    "    if wrong_str :\n",
    "        print('get_wrong_str...')\n",
    "        print(repr(text))\n",
    "        print(repr(pred))\n",
    "        print(repr(selected_text))\n",
    "        print(wrong_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, args):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_avg_loss = []\n",
    "    val_acc_score = []\n",
    "    valid_preds = []\n",
    "#     valid_preds_df = pd.DataFrame(columns=['text', 'selected_text',\n",
    "#                                            'pred_text', 'offset',\n",
    "#                                            'start_logits','end_logits',\n",
    "#                                            'pred_start', 'pred_end',\n",
    "#                                            'read_start','read_end'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader, desc=\"Evaluating\")):\n",
    "            \n",
    "            if args.is_cuda:\n",
    "                for key,value in batch.items():\n",
    "                    if isinstance(value,torch.Tensor):\n",
    "                        batch[key] = batch[key].to(device)\n",
    "#                         batch[key] = batch[key].cuda()\n",
    "            input_ids, input_masks, input_segments, ori_start, ori_end= (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"token_type_ids\"],\n",
    "                batch[\"start_position\"],\n",
    "                batch[\"end_position\"],\n",
    "            )\n",
    "\n",
    "    \n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=input_ids, attention_mask=input_masks, token_type_ids=input_segments,\n",
    "            )\n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(start_logits, end_logits, ori_start, ori_end)\n",
    "            val_avg_loss.append(loss.item())\n",
    "            \n",
    "            # score\n",
    "            pred_start = F.softmax(start_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "            pred_end = F.softmax(end_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "\n",
    "            ori_start = ori_start.cpu().data.numpy()\n",
    "            ori_end = ori_end.cpu().data.numpy()\n",
    "            offset_mapping = batch['offset_mapping'].cpu().data.numpy()\n",
    "            \n",
    "            for exam_idx in range(ori_start.shape[0]):\n",
    "                pred_str = get_output_string(batch['text'][exam_idx],\n",
    "                                              offset_mapping[exam_idx],\n",
    "                                              pred_start[exam_idx],\n",
    "                                              pred_end[exam_idx])\n",
    "                score = jaccard(pred_str,batch['selected_text'][exam_idx])\n",
    "#                 if rn.random()>0.95 and score<0.7:\n",
    "#                     print('-----------------preds sample--------------------')\n",
    "#                     print(score)\n",
    "#                     print('Text:',batch['text'][exam_idx])\n",
    "#                     print('Preds:',pred_str)\n",
    "#                     print('Corec:',batch['selected_text'][exam_idx])\n",
    "                    \n",
    "                off_set = [batch['text'][exam_idx][offset[0] : offset[1]] for offset in offset_mapping[exam_idx]]\n",
    "                start_logit = [max(0,round(i,2)) for i in start_logits[exam_idx].cpu().data.numpy()] \n",
    "                c = [' /'.join([str(string),str(score)]) for score,string in zip(start_logit,off_set)]\n",
    "                read_start = '   '.join(c)\n",
    "                end_logit = [max(0,round(i,2)) for i in end_logits[exam_idx].cpu().data.numpy()]\n",
    "                c = [' /'.join([str(string),str(score)]) for score,string in zip(end_logit,off_set)]\n",
    "                read_end = '   '.join(c)\n",
    "                \n",
    "                valid_preds.append({'text':batch['text'][exam_idx],\n",
    "                                    'selected_text':batch['selected_text'][exam_idx],\n",
    "                                    'pred_text':pred_str,\n",
    "                                    'offset':offset_mapping[exam_idx].tolist(),\n",
    "                                    'start_logits':start_logit,\n",
    "                                    'end_logits':end_logit,\n",
    "                                    'pred_start':pred_start[exam_idx], \n",
    "                                    'pred_end':pred_end[exam_idx],\n",
    "                                    'read_start':read_start,\n",
    "                                    'read_end':read_end,\n",
    "                                   })\n",
    "\n",
    "#                 valid_preds_df.loc[valid_preds_df.shape[0]] = [batch['text'][exam_idx],\n",
    "#                                                                batch['selected_text'][exam_idx], \n",
    "#                                                                pred_str, \n",
    "#                                                                offset_mapping[exam_idx].tolist(), \n",
    "#                                                                start_logit,\n",
    "#                                                                end_logit,\n",
    "#                                                                pred_start[exam_idx], \n",
    "#                                                                pred_end[exam_idx],\n",
    "#                                                                read_start,\n",
    "#                                                                read_end,]\n",
    "                val_acc_score.append(score)\n",
    "\n",
    "    val_avg_loss = round(sum(val_avg_loss)/len(val_avg_loss),4)\n",
    "    val_acc_score = round(sum(val_acc_score)/len(val_acc_score),4)\n",
    "    valid_preds_df = pd.DataFrame(valid_preds)\n",
    "    if args.is_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return val_avg_loss, val_acc_score, valid_preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def infer(model, data_loader, args):\n",
    "    model.eval()\n",
    "\n",
    "    test_preds = []\n",
    "    \n",
    "    test_preds_df = pd.DataFrame(columns=['textID','selected_text'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader, desc=\"Infering\")):\n",
    "            \n",
    "            if args.is_cuda:\n",
    "                for key,value in batch.items():\n",
    "                    if isinstance(value,torch.Tensor):\n",
    "                        batch[key] = batch[key].to(device)\n",
    "\n",
    "            input_ids, input_masks, input_segments = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"token_type_ids\"],\n",
    "            )\n",
    "    \n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=input_ids, attention_mask=input_masks, token_type_ids=input_segments,\n",
    "            )\n",
    "        \n",
    "            pred_start = F.softmax(start_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "            pred_end = F.softmax(end_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "            \n",
    "            offset_mapping = batch['offset_mapping'].cpu().data.numpy()\n",
    "            \n",
    "            for exam_idx in range(pred_start.shape[0]):\n",
    "                pred_str = get_output_string(batch['text'][exam_idx],\n",
    "                                              offset_mapping[exam_idx],\n",
    "                                              pred_start[exam_idx],\n",
    "                                              pred_end[exam_idx])\n",
    "                \n",
    "                test_preds_df.loc[test_preds_df.shape[0]] = [batch['textID'][exam_idx], pred_str]\n",
    "                \n",
    "    if args.is_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return test_preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_split(train_df, args):\n",
    "    \n",
    "    gkf = StratifiedKFold(n_splits=args.n_splits).split(X=train_df, y=train_df.sentiment)\n",
    "             \n",
    "    for fold, (train_index, val_index) in enumerate(gkf):\n",
    "        print('fold: ',fold)\n",
    "        print(train_index[:5])\n",
    "        print(val_index[:5])\n",
    "        \n",
    "        train_dataset = SpanDataset(args.TOKENIZER,\n",
    "                                    train_df.iloc[train_index],\n",
    "                                    args.max_seq_length, \n",
    "                                    is_raw=True, )\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  shuffle=True,\n",
    "                                  batch_size=args.batch, \n",
    "                                  num_workers=2,)   \n",
    "        valid_dataset = SpanDataset(args.TOKENIZER,\n",
    "                                    train_df.iloc[val_index],\n",
    "                                    args.max_seq_length, \n",
    "                                    is_raw=True, )\n",
    "        valid_loader = DataLoader(valid_dataset, \n",
    "                                  shuffle=False,\n",
    "                                  batch_size=args.batch, \n",
    "                                  num_workers=2,)\n",
    "        \n",
    "        yield fold, train_loader, valid_loader, train_index, val_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, data_loader, optimizer, criterion, scheduler, iteration, args):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    avg_loss = []\n",
    "    acc_score = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, desc=\"Training\")):\n",
    "        if args.is_cuda:\n",
    "            for key,value in batch.items():\n",
    "                if isinstance(value,torch.Tensor):\n",
    "                    batch[key] = batch[key].to(device)\n",
    "\n",
    "        input_ids, input_masks, input_segments, ori_start, ori_end= (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"token_type_ids\"],\n",
    "            batch[\"start_position\"],\n",
    "            batch[\"end_position\"],\n",
    "        )\n",
    "\n",
    "        start_logits, end_logits = model(\n",
    "            input_ids=input_ids, attention_mask=input_masks, token_type_ids=input_segments,\n",
    "        )\n",
    "#         if idx==9:\n",
    "#             print([round(i,2) for i in start_logits[0].cpu().data.numpy()])\n",
    "#             print([round(i,2) for i in end_logits[0].cpu().data.numpy()])\n",
    "        # loss\n",
    "        loss = criterion(start_logits, end_logits, ori_start, ori_end)\n",
    "        loss.backward()\n",
    "        \n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        # optim\n",
    "        if (iteration + 1) % args.batch_accumulation == 0:  # 延迟更新参数，增加batch_size\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "#                 if iteration==6:print('schedule step')\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        iteration += 1\n",
    "    \n",
    "        # score\n",
    "#         pred_start = F.softmax(start_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "#         pred_end = F.softmax(end_logits,dim=1).argmax(dim=-1).cpu().data.numpy()\n",
    "\n",
    "#         ori_start = ori_start.cpu().data.numpy()\n",
    "#         ori_end = ori_end.cpu().data.numpy()\n",
    "#         offset_mapping = batch['offset_mapping'].cpu().data.numpy()\n",
    "\n",
    "#         for exam_idx in range(ori_start.shape[0]):\n",
    "#             pred_str = get_output_string(batch['text'][exam_idx],\n",
    "#                                           offset_mapping[exam_idx],\n",
    "#                                           pred_start[exam_idx],\n",
    "#                                           pred_end[exam_idx])\n",
    "#             acc_score.append(\n",
    "#                 jaccard(pred_str,batch['selected_text'][exam_idx]))\n",
    "    \n",
    "    avg_loss = round(sum(avg_loss)/len(avg_loss),4)\n",
    "#     acc_score = round(sum(acc_score)/len(acc_score),4)\n",
    "    acc_score = 0.0\n",
    "    # 清理\n",
    "    if args.is_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return avg_loss, acc_score, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(args,train_df,test_df):\n",
    "    \n",
    "    seed_everything(8421)\n",
    "    print('model path ... ',args.bert_model)\n",
    "    chart_df = pd.DataFrame(columns=['fold', 'epoch', 'avg_loss', 'acc_score','val_avg_loss', 'val_acc_score'])\n",
    "\n",
    "    for fold, train_loader, valid_loader, train_index, val_index in cross_validation_split(train_df, args):\n",
    "        if fold not in args.fold:\n",
    "            continue\n",
    "\n",
    "        model = SpanBert(config = Model_Class[args.model_name][0].from_pretrained(args.bert_model),\n",
    "                         model = Model_Class['roberta'][2],\n",
    "                         PTM_path = args.bert_model)\n",
    "        \n",
    "        if torch.cuda.device_count() > 1: \n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "        if args.is_cuda:\n",
    "            model.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer = get_model_optimizer(model,args)\n",
    "        criterion = loss_fn\n",
    "        \n",
    "        fold_checkpoints = os.path.join(args.checkpoints_path, \"model_{}_{}_{}_{}\".format(args.model_name,fold,args.lr,args.batch))\n",
    "        fold_predictions = os.path.join(args.predictions_path, \"model_{}_{}_{}_{}\".format(args.model_name,fold,args.lr,args.batch))\n",
    "        os.makedirs(fold_checkpoints, exist_ok=True)\n",
    "        os.makedirs(fold_predictions, exist_ok=True)\n",
    "\n",
    "        iteration=0\n",
    "        best_score = 0.0\n",
    "        results = []\n",
    "        \n",
    "        if not args.is_scheduler:\n",
    "            scheduler=None\n",
    "            print('not schedule')\n",
    "        else:\n",
    "            print('has schedule')\n",
    "            sche_step = args.epochs * ceil(len(train_index)/args.batch) / args.batch_accumulation\n",
    "            warmup_steps = sche_step//3\n",
    "            print('schedule step:',sche_step)\n",
    "            print('schedule warm up :',warmup_steps)\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=warmup_steps,\n",
    "                num_training_steps=sche_step,\n",
    "            )\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            print(np.around(model.liner_to_start.weight.var(dim=1).cpu().data.numpy(),6))\n",
    "            print(np.around(model.liner_to_end.weight.var(dim=1).cpu().data.numpy(),6))\n",
    "            \n",
    "            avg_loss, acc_score, iteration = train_loop(\n",
    "                model, train_loader, optimizer, criterion, scheduler, iteration, args)\n",
    "\n",
    "            val_avg_loss, val_acc_score, valid_preds_df = evaluate(\n",
    "                model, valid_loader, criterion, args)\n",
    "\n",
    "            print(\"Epoch {}/{}:  loss={:.3f} score={:.3f} val_loss={:.3f} val_score={:.3f} \".format(\n",
    "                    epoch + 1, args.epochs, avg_loss, acc_score, val_avg_loss, val_acc_score))  \n",
    "      \n",
    "            chart_df.loc[chart_df.shape[0]] = [fold, epoch, avg_loss, acc_score, val_avg_loss, val_acc_score]\n",
    "            \n",
    "            if val_acc_score > best_score and args.is_save:\n",
    "                best_score = val_acc_score\n",
    "                torch.save( model.state_dict(), os.path.join(fold_checkpoints, \"best_model.pth\"))\n",
    "                valid_preds_df.to_csv(os.path.join(fold_predictions, \"best_preds.csv\"),index=False)\n",
    "                \n",
    "    del model, optimizer, criterion, scheduler\n",
    "    del valid_loader, train_loader, #test_loader\n",
    "    if args.is_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return chart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "#     multi_task_balance = 0.5\n",
    "    fold=[0,1,2,3,4,] # \n",
    "    lr=3e-5\n",
    "    epochs=6\n",
    "     \n",
    "    model_name = 'roberta'\n",
    "    batch = 72\n",
    "    batch_accumulation = 1\n",
    "    \n",
    "    max_seq_length = 112\n",
    "#     warmup_steps = 30\n",
    "    n_splits = 5\n",
    "#     bert_model='/kaggle/input/robertalargehugging-face'\n",
    "    bert_model='/kaggle/input/roberta-base'\n",
    "#     bert_model = '/kaggle/input/bert-base-uncased'\n",
    "    is_cuda=torch.cuda.is_available()\n",
    "    predictions_path=\"prediction_dir\"\n",
    "    checkpoints_path=\"model_dir\"\n",
    "    \n",
    "    is_scheduler = True\n",
    "    is_finetune_code = False\n",
    "    is_save=True\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "        vocab_file=f\"{bert_model}/vocab.json\", \n",
    "        merges_file=f\"{bert_model}/merges.txt\", \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "# argsa()   \n",
    "# args.lr\n",
    "device = torch.device(\"cuda\")\n",
    "Model_Class = {'bert':[BertConfig, BertTokenizerFast, BertModel],\n",
    "              'roberta':[RobertaConfig, RobertaTokenizerFast, RobertaModel]}\n",
    "\n",
    "\n",
    "# CheckPoint = [\n",
    "#                 '/kaggle/input/sentiment-extraction-roberta-0/model_dir/model_roberta_7/best_model.pth',\n",
    "#                 '/kaggle/input/sentiment-extraction-roberta-0/model_dir/model_roberta_4/best_model.pth',\n",
    "#                 '/kaggle/input/sentiment-extraction-roberta-0/model_dir/model_roberta_1/best_model.pth',\n",
    "#              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if args.is_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "if args.is_finetune_code:\n",
    "    chart_df = main(args,train_df.iloc[:3000],test_df)\n",
    "else:\n",
    "    chart_df= main(args,train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(ncols=2,figsize=(16, 8))\n",
    "sns.lineplot(data=chart_df[['avg_loss', 'val_avg_loss']], hue =chart_df['fold'], ax=axes[0]);\n",
    "sns.lineplot(data=chart_df[['acc_score', 'val_acc_score']], hue =chart_df['fold'], ax=axes[1]);\n",
    "chart_df.to_csv('verbose_chart.csv',index=False)\n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df.groupby('fold')['avg_loss'].min(),\\\n",
    "chart_df.groupby('fold')['avg_loss'].min().mean(),\\\n",
    "chart_df.groupby('fold')['val_avg_loss'].min(),\\\n",
    "chart_df.groupby('fold')['val_avg_loss'].min().mean(),\\\n",
    "chart_df.groupby('fold')['val_acc_score'].max(),\\\n",
    "chart_df.groupby('fold')['val_acc_score'].max().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
